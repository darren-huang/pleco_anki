{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f312a93c",
   "metadata": {},
   "source": [
    "# Define Input/Output/Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad71622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars ###################################\n",
    "from opencc import OpenCC\n",
    "\n",
    "toTrad = OpenCC(\"s2tw\")\n",
    "toSimp = OpenCC(\"t2s\")\n",
    "SEPARATOR = \"\\x1f\"\n",
    "\n",
    "# input vars #####################################\n",
    "\n",
    "# folders\n",
    "WORDLISTS = \"wordlists\"\n",
    "APKGS = \"apkgs\"\n",
    "EXPERIMENTS = f\"{APKGS}/experiments\"\n",
    "OUTPUTS = \"outputs\"\n",
    "\n",
    "# files\n",
    "hsk3_wordlist_file = f\"{WORDLISTS}/final_formatted_wordlist.txt\"\n",
    "pleco_imported_apkg_file = f\"{APKGS}/backfilled_pleco_imports_v2.1.apkg\"\n",
    "original_deck_name = \"Pleco Import\"\n",
    "new_deck_name = \"HSK 3.0 (Pleco Definitions)\"\n",
    "extracted_anki_sqlite_folder = f\"{EXPERIMENTS}/backfilled_pleco_imports_v2.1\"\n",
    "output_apkg_file = f\"{OUTPUTS}/pleco_import_backfilled_direct_sqlitev2.1.apkg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70895cd0",
   "metadata": {},
   "source": [
    "# Load HSK3 Wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a3f05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word entry class\n",
    "from typing import List\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class WordEntry:\n",
    "    sort_numbers: List[int]\n",
    "    tags: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de4f6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process wordlist into dictionary\n",
    "hsk3_wordlist = {}\n",
    "n_words_in_wordlist, n_dup = 0, 0\n",
    "current_tag = \"\"\n",
    "for line in open(hsk3_wordlist_file, encoding=\"utf-8\"):\n",
    "    line = line.strip()\n",
    "    if \"hsk_level\" in line:\n",
    "        current_tag = line.replace(\"hsk_l\", \"HSK_L\")\n",
    "        continue\n",
    "    else:\n",
    "        trad_char = toTrad.convert(line)\n",
    "        if trad_char not in hsk3_wordlist:\n",
    "            hsk3_wordlist[trad_char] = WordEntry(\n",
    "                sort_numbers=[n_words_in_wordlist], tags=[current_tag]\n",
    "            )\n",
    "        else:\n",
    "            n_dup += 1\n",
    "            hsk3_wordlist[trad_char].sort_numbers.append(n_words_in_wordlist)\n",
    "            hsk3_wordlist[trad_char].tags.append(current_tag)\n",
    "        n_words_in_wordlist += 1\n",
    "\n",
    "hsk3_wordlist_set = set(hsk3_wordlist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40ab9db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('愛', WordEntry(sort_numbers=[0], tags=['HSK_Level_1']))\n",
      "('愛好', WordEntry(sort_numbers=[1], tags=['HSK_Level_1']))\n",
      "('八', WordEntry(sort_numbers=[2], tags=['HSK_Level_1']))\n",
      "('爸爸', WordEntry(sort_numbers=[3], tags=['HSK_Level_1']))\n",
      "('吧', WordEntry(sort_numbers=[4], tags=['HSK_Level_1']))\n",
      "('白', WordEntry(sort_numbers=[5, 1275], tags=['HSK_Level_1', 'HSK_Level_3']))\n",
      "('白天', WordEntry(sort_numbers=[6], tags=['HSK_Level_1']))\n",
      "('百', WordEntry(sort_numbers=[7], tags=['HSK_Level_1']))\n",
      "('班', WordEntry(sort_numbers=[8], tags=['HSK_Level_1']))\n",
      "('半', WordEntry(sort_numbers=[9], tags=['HSK_Level_1'])) \n",
      "...\n",
      "\n",
      "current wordlist is 'wordlists/final_formatted_wordlist.txt'\n",
      "number of words inside original wordlist: 11073\n",
      "number of unique words:                   10921\n",
      "num_duplicates:                           152\n"
     ]
    }
   ],
   "source": [
    "# print stats on wordlist dict\n",
    "print(\"\\n\".join([str(t) for t in list(hsk3_wordlist.items())[:10]]), \"\\n...\\n\")\n",
    "\n",
    "print(f\"current wordlist is '{hsk3_wordlist_file}'\")\n",
    "print(\"number of words inside original wordlist:\", n_words_in_wordlist)\n",
    "print(\"number of unique words:                  \", len(hsk3_wordlist_set))\n",
    "print(\"num_duplicates:                          \", n_dup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35762839",
   "metadata": {},
   "source": [
    "# Connect to SQLite DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37e69707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup experiments folder (.apkg are compressed sqlite databases, this just unzips it)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.isdir(extracted_anki_sqlite_folder):\n",
    "    # delete if target experiment folder already exists\n",
    "    shutil.rmtree(extracted_anki_sqlite_folder)\n",
    "\n",
    "\n",
    "with ZipFile(pleco_imported_apkg_file) as zf:\n",
    "    # unzip apkg to target folder\n",
    "    zf.extractall(extracted_anki_sqlite_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfe86c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup sqlite db connection\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(f\"{extracted_anki_sqlite_folder}/collection.anki2\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59c5ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('col',), ('notes',), ('cards',), ('revlog',), ('sqlite_stat1',), ('sqlite_stat4',), ('graves',)]\n"
     ]
    }
   ],
   "source": [
    "# show all tables\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b41e1d89",
   "metadata": {},
   "source": [
    "# missing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8ca20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_trad_word(simp_word):\n",
    "    return toTrad.convert(toSimp.convert(simp_word))\n",
    "\n",
    "\n",
    "def get_trad_word_from_row(row):\n",
    "    _id, flds = row\n",
    "    flds_arr = flds.split(SEPARATOR)\n",
    "    return get_trad_word(flds_arr[0])\n",
    "\n",
    "\n",
    "sqlite_wordset = set()\n",
    "\n",
    "\n",
    "def refresh_sqlite_wordset():\n",
    "    global sqlite_wordset\n",
    "    sqlite_wordset = set(\n",
    "        [\n",
    "            get_trad_word_from_row(row)\n",
    "            for row in list(cur.execute(\"SELECT id, flds FROM notes;\"))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f0afcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add exceptions\n",
    "variants = [(\"燻\", \"薰\")]\n",
    "exceptions = (\n",
    "    {}\n",
    ")  # maps toTrad.convert(hsk3_simp_word) <-> anki_sqlite front field (traditional)\n",
    "for w1, w2 in variants:\n",
    "    exceptions[w1], exceptions[w2] = w2, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf6c0c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "words inside anki sqlite cards: 10926\n",
      "words inside hsk3 wordlist:     10921\n",
      "misfits (in anki, but not hsk3 wordlist): 5\n",
      "missing (in hsk 3wordlist, but not anki): 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract words present inside anki apkg & compare to the hsk3 wordlist\n",
    "\n",
    "refresh_sqlite_wordset()\n",
    "misfits = sqlite_wordset - hsk3_wordlist_set\n",
    "missing = hsk3_wordlist_set - sqlite_wordset\n",
    "\n",
    "# process variants/exceptions\n",
    "for miss in missing.copy():\n",
    "    if miss in exceptions:\n",
    "        missing.remove(miss)\n",
    "        misfits.remove(exceptions[miss])\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "words inside anki sqlite cards: {len(sqlite_wordset)}\n",
    "words inside hsk3 wordlist:     {len(hsk3_wordlist_set)}\n",
    "misfits (in anki, but not hsk3 wordlist): {len(misfits)}\n",
    "missing (in hsk 3wordlist, but not anki): {len(missing)}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c329455",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num misfits (in anki but not in hsk3 wordlist): 5\n",
      "simplified:  ['怀', '见过世面', '揹', '不难理会', '指著和尚骂秃子']\n",
      "traditional: ['懷', '見過世面', '揹', '不難理會', '指著和尚罵禿子']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze misfits\n",
    "print(\n",
    "    f\"\"\"\n",
    "num misfits (in anki but not in hsk3 wordlist): {len(misfits)}\n",
    "simplified:  {[toSimp.convert(word) for word in list(misfits)[:10]]}\n",
    "traditional: {list(misfits)[:10]}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb6f3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num missing (in hsk3 wordlist but not in anki): 0\n",
      "simplified:  []\n",
      "traditional: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing\n",
    "print(\n",
    "    f\"\"\"\n",
    "num missing (in hsk3 wordlist but not in anki): {len(missing)}\n",
    "simplified:  {[toSimp.convert(word) for word in list(missing)[:10]]}\n",
    "traditional: {list(missing)[:10]}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c192a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write missing words to a txt file\n",
    "\n",
    "# backfill_todo_wordlist_file = \"backfill_todo_wordlist_file.txt\"\n",
    "# with open(backfill_todo_wordlist_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"\\n\".join(sorted(list(missing))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0c9be2f",
   "metadata": {},
   "source": [
    "# Delete Misfit Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98c1ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def delete_words(cur, words):\n",
    "    word_to_id = {\n",
    "        get_trad_word_from_row(row): row[0]\n",
    "        for row in list(cur.execute(\"SELECT id, flds FROM notes;\"))\n",
    "    }\n",
    "    for word in words:\n",
    "        del_id = word_to_id[word]\n",
    "        cur.execute(\"DELETE FROM notes WHERE id=?\", (del_id,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74d06c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite wordset length: 10926\n",
      "deleting misfit words...\n",
      "sqlite wordset length: 10921\n"
     ]
    }
   ],
   "source": [
    "# delete misfit words and print out stats\n",
    "\n",
    "print(\"sqlite wordset length:\", len(sqlite_wordset))\n",
    "print(\"deleting misfit words...\")\n",
    "\n",
    "delete_words(cur, misfits)\n",
    "refresh_sqlite_wordset()\n",
    "\n",
    "print(\"sqlite wordset length:\", len(sqlite_wordset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5654ab2",
   "metadata": {},
   "source": [
    "# Update New Fields for Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d406e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import re\n",
    "import html\n",
    "\n",
    "DEFINITION_PATT = r\"\"\"[\\s\\S]*?<\\/div>(<div align=\"left\">[\\s\\S]*?<\\/div>)\"\"\"\n",
    "BACK_PATT = r\"(^[\\s\\S]+〔)\\S+(〕[\\s\\S]+)\"\n",
    "\n",
    "\n",
    "def back_to_simplified(front, back):\n",
    "    back = html.unescape(back)\n",
    "    match = re.match(BACK_PATT, back)\n",
    "    if match:\n",
    "        beginning, end = match.group(1), match.group(2)\n",
    "        beginning, end = toSimp.convert(beginning), toSimp.convert(end)\n",
    "        return beginning + front.strip() + end\n",
    "    elif \"〔\" not in back and \"〕\" not in back:\n",
    "        return toSimp.convert(back)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid back format: {back}\")\n",
    "\n",
    "\n",
    "def extract_pinyin(back):\n",
    "    \"\"\"Extract pinyin from card definition html.\"\"\"\n",
    "    x = re.match(r\"\"\"[\\s\\S]*?<br>([\\s\\S]+?>PY <[\\s\\S]+?<\\/span>)<\\/p>\"\"\", back).group(1)\n",
    "    x = re.sub(\"<.*?>\", \"\", x)\n",
    "    x = re.sub(\"//\", \"\", x)\n",
    "    x = re.sub(\"PY\", \"\", x)\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "\n",
    "def extract_definition(back):\n",
    "    match = re.match(DEFINITION_PATT, back)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        raise Exception(f\"Bad definition formatting: {back} {len(back)}\")\n",
    "\n",
    "\n",
    "def process_flds(flds):\n",
    "    \"\"\"Given anki fields, extract separate fields & combine with hsk3 tagging information.\n",
    "\n",
    "    v2.0 card type has the following fields\n",
    "    Front, Back, Back_simplified, traditional, simplified, definition, definition_simplified, pinyin, learning_order, metadata\n",
    "\n",
    "    note the following fields must be html escaped:\n",
    "    Back, back_simplified, definition, definition_simplified\n",
    "    \"\"\"\n",
    "    flds_arr = flds.split(SEPARATOR)\n",
    "    front, back = flds_arr[0], flds_arr[1]\n",
    "    traditional = get_trad_word(front)\n",
    "    simplified = toSimp.convert(traditional)\n",
    "    back_simplified = back_to_simplified(front, back)\n",
    "\n",
    "    word_entry = (\n",
    "        hsk3_wordlist[traditional]\n",
    "        if traditional in hsk3_wordlist\n",
    "        else hsk3_wordlist[exceptions[traditional]]\n",
    "    )\n",
    "    definition = extract_definition(back)\n",
    "    definition_simplified = extract_definition(back_simplified)\n",
    "    pinyin = extract_pinyin(back)\n",
    "    learning_order = str(min(word_entry.sort_numbers))\n",
    "    metadata = str(word_entry)\n",
    "    new_tags = f\" {' '.join(word_entry.tags)} \"\n",
    "\n",
    "    new_flds = SEPARATOR.join(\n",
    "        [\n",
    "            front,\n",
    "            back,\n",
    "            back_simplified,\n",
    "            traditional,\n",
    "            simplified,\n",
    "            definition,\n",
    "            definition_simplified,\n",
    "            pinyin,\n",
    "            learning_order,\n",
    "            metadata,\n",
    "        ]\n",
    "    )\n",
    "    return new_flds, new_tags\n",
    "\n",
    "\n",
    "def update(cur, _id, new_flds, new_tags):\n",
    "    \"\"\"Update fields and tags.\"\"\"\n",
    "    cur.execute(\n",
    "        \"UPDATE notes SET flds = ?, tags = ? WHERE id = ?\", (new_flds, new_tags, _id)\n",
    "    )\n",
    "\n",
    "\n",
    "def process_row(cur, row):\n",
    "    \"\"\"For the given row, generates new fields and updates the sqlite db.\"\"\"\n",
    "    _id, flds = row\n",
    "    new_flds, new_tags = process_flds(flds)\n",
    "    if new_flds.count(SEPARATOR) != flds.count(SEPARATOR):\n",
    "        raise Exception(\"Generated fields do not match the number of old fields\")\n",
    "    update(cur, _id, new_flds, new_tags)\n",
    "\n",
    "\n",
    "def update_all(cur):\n",
    "    \"\"\"Runs the process on all rows.\"\"\"\n",
    "    for row in list(cur.execute(\"SELECT id, flds FROM notes;\")):\n",
    "        process_row(cur, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab4037e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update all rows\n",
    "update_all(cur)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d5a0db9",
   "metadata": {},
   "source": [
    "# Update Deck Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31442f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update deck name from 'Pleco Import' to 'HSK 3.0 (Pleco Definitions)'\n"
     ]
    }
   ],
   "source": [
    "# Update the saved deck name\n",
    "import json\n",
    "\n",
    "# extract collection information\n",
    "collections = list(cur.execute(\"SELECT id, decks FROM col;\"))\n",
    "assert len(collections) == 1\n",
    "col_id, decks_json = collections[0]\n",
    "decks = json.loads(decks_json)\n",
    "\n",
    "# retrieve target deck object and rename\n",
    "found = False\n",
    "for deck_id, deck in decks.items():\n",
    "    if original_deck_name in deck[\"name\"]:\n",
    "        deck[\"name\"] = deck[\"name\"].replace(original_deck_name, new_deck_name)\n",
    "        found = True\n",
    "\n",
    "if not found:\n",
    "    raise Exception(f\"Deck '{original_deck_name}' not found in anki collection\")\n",
    "\n",
    "# update database\n",
    "cur.execute(\"UPDATE col SET decks= ? WHERE id = ?\", (json.dumps(decks), col_id))\n",
    "print(f\"Update deck name from '{original_deck_name}' to '{new_deck_name}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e172794b",
   "metadata": {},
   "source": [
    "# Commit to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af66da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da6d1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eac39a98",
   "metadata": {},
   "source": [
    "# Zip to Output Apkg File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2b24420",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(output_apkg_file, \"zip\", extracted_anki_sqlite_folder)\n",
    "os.rename(output_apkg_file + \".zip\", output_apkg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de8c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
